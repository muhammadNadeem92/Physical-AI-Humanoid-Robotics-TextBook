"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[92],{1146(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-08-capstone-autonomous-humanoid/system-architecture","title":"Chapter 1: System Architecture & Data Flow","description":"Introduction","source":"@site/docs/module-08-capstone-autonomous-humanoid/system-architecture.md","sourceDirName":"module-08-capstone-autonomous-humanoid","slug":"/module-08-capstone-autonomous-humanoid/system-architecture","permalink":"/docs/module-08-capstone-autonomous-humanoid/system-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadNadeem92/physical-ai-humanoid-robotics-textbook/tree/main/docs/module-08-capstone-autonomous-humanoid/system-architecture.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Human\u2013Robot Interaction (HRI)","permalink":"/docs/module-07-humanoid-hri/human-robot-interaction"},"next":{"title":"Chapter 2: Voice-to-Plan Pipeline","permalink":"/docs/module-08-capstone-autonomous-humanoid/voice-to-plan"}}');var s=t(4848),o=t(8453);const a={},r="Chapter 1: System Architecture & Data Flow",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Perception \u2192 Planning \u2192 Action Loop",id:"perception--planning--action-loop",level:3},{value:"Component Isolation",id:"component-isolation",level:3},{value:"Deterministic vs Probabilistic Modules",id:"deterministic-vs-probabilistic-modules",level:3},{value:"Examples",id:"examples",level:2},{value:"Example: End-to-End System Diagram",id:"example-end-to-end-system-diagram",level:3},{value:"Example: Message Flow Between Components",id:"example-message-flow-between-components",level:3},{value:"Example: Safety Boundary Implementation",id:"example-safety-boundary-implementation",level:3},{value:"Summary &amp; Key Takeaways",id:"summary--key-takeaways",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-1-system-architecture--data-flow",children:"Chapter 1: System Architecture & Data Flow"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"This chapter defines the full Physical AI stack for the capstone autonomous humanoid system. You'll learn how to integrate all previous modules into a cohesive architecture that connects voice processing, LLM reasoning, vision perception, and ROS 2 action execution. The focus is on system-level understanding, component isolation, and safety boundaries that ensure reliable operation."}),"\n",(0,s.jsxs)(n.p,{children:["The autonomous humanoid system represents the culmination of embodied intelligence concepts from all previous modules. This chapter builds on the ROS 2 architecture from ",(0,s.jsx)(n.a,{href:"../module-02-ros2/architecture-setup",children:"Module 2"}),", the simulation fundamentals from ",(0,s.jsx)(n.a,{href:"../module-03-sim-fundamentals/robot-description-models",children:"Module 3"}),", the NVIDIA Isaac platform concepts from ",(0,s.jsx)(n.a,{href:"../module-05-isaac-ai-brain/isaac-architecture",children:"Module 5"}),", the Vision-Language-Action systems from ",(0,s.jsx)(n.a,{href:"../module-06-vla-systems/vla-foundations",children:"Module 6"}),", and the humanoid systems from ",(0,s.jsx)(n.a,{href:"../module-07-humanoid-hri/kinematics-dynamics",children:"Module 7"}),". This chapter establishes the architectural foundation that enables all subsequent components to work together safely and effectively. You'll understand how to design systems with proper separation of concerns while maintaining the ability to handle complex, real-world scenarios."]}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you'll understand how to design a complete Physical AI architecture with appropriate component isolation, safety boundaries, and data flow patterns that connect voice, vision, and action in a cohesive system."}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"perception--planning--action-loop",children:"Perception \u2192 Planning \u2192 Action Loop"}),"\n",(0,s.jsx)(n.p,{children:"The fundamental pattern that connects all system components in a continuous cycle of sensing, reasoning, and acting."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Loop:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception"}),": Environmental sensing through vision and other sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Planning"}),": High-level reasoning and task decomposition using LLMs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Execution of plans through ROS 2 actions and navigation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback"}),": Results from actions feed back to perception and planning"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Characteristics:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Asynchronous"}),": Components operate at different frequencies and time scales"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Probabilistic"}),": Perception and planning components handle uncertainty"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deterministic"}),": Action components execute with predictable timing when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safe"}),": Each component has safety boundaries and fail-safe mechanisms"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"component-isolation",children:"Component Isolation"}),"\n",(0,s.jsx)(n.p,{children:"The architectural principle that separates system components to maintain stability and safety when individual components fail or behave unpredictably."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Isolation Layers:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Isolation"}),": Voice and vision inputs are processed separately before integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Processing Isolation"}),": LLM reasoning operates independently from perception processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution Isolation"}),": ROS 2 actions execute independently with their own safety checks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring Isolation"}),": Each component has independent monitoring and health checks"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Benefits:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fault Tolerance"}),": Failure in one component doesn't cascade to others"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Maintainability"}),": Components can be updated or replaced independently"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scalability"}),": Components can be optimized separately"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"}),": Clear boundaries for safety checks and fail-safe mechanisms"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"deterministic-vs-probabilistic-modules",children:"Deterministic vs Probabilistic Modules"}),"\n",(0,s.jsx)(n.p,{children:"The architectural distinction between components that provide predictable outputs and those that handle uncertainty."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Deterministic Modules:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Action Execution"}),": Given the same inputs, produces the same outputs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Boundary Checks"}),": Clear pass/fail conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State Machine Transitions"}),": Predictable state changes based on inputs"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Probabilistic Modules:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision Perception"}),": Outputs have confidence levels and uncertainty"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM Reasoning"}),": May produce different outputs for similar inputs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Processing"}),": Ambiguity and interpretation variability"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Integration Strategy:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validation Layers"}),": Probabilistic outputs are validated before use"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback Mechanisms"}),": Deterministic alternatives when probabilistic components fail"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Uncertainty Propagation"}),": Uncertainty is tracked and managed through the system"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.h3,{id:"example-end-to-end-system-diagram",children:"Example: End-to-End System Diagram"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[Voice Command] \u2192 [Speech-to-Text] \u2192 [LLM Intent Parser] \u2192 [Task Planner]\r\n       \u2193              \u2193                    \u2193                 \u2193\r\n[Voice Safety]   [Text Safety]      [Intent Safety]   [Plan Safety]\r\n       \u2193              \u2193                    \u2193                 \u2193\r\n[Environment] \u2190 [Perception] \u2190 [World Model] \u2190 [Plan Validator]\r\n     \u2193              \u2193               \u2193              \u2193\r\n[Navigate] \u2192 [Action Executor] \u2192 [Monitor] \u2192 [Feedback]\r\n     \u2193              \u2193               \u2193              \u2193\r\n[Safety]      [ROS 2 Actions]  [State]      [Adaptation]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"example-message-flow-between-components",children:"Example: Message Flow Between Components"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'1. Voice Input: "Clean the room: navigate to the table, pick up the bottle, place in bin"\r\n   \u2192 Safety Check: Valid command format \u2713\r\n   \u2192 Speech-to-Text: "clean the room: navigate to the table, pick up the bottle, place in bin"\r\n\r\n2. Intent Parsing:\r\n   \u2192 Extract Task: Navigation \u2192 Manipulation \u2192 Placement\r\n   \u2192 Identify Objects: table, bottle, bin\r\n   \u2192 Safety Check: Objects exist in environment \u2713\r\n\r\n3. Perception Request:\r\n   \u2192 "Locate table, bottle, bin in current view"\r\n   \u2192 Vision System: "table at (2.1, 0.5), bottle at (2.2, 0.6), bin at (1.8, -0.3)"\r\n\r\n4. Plan Validation:\r\n   \u2192 Check Navigation Feasibility: "Path to table is clear \u2713"\r\n   \u2192 Check Manipulation Feasibility: "Bottle is graspable \u2713"\r\n   \u2192 Check Placement Feasibility: "Bin has space \u2713"\r\n\r\n5. Action Execution:\r\n   \u2192 Navigate to table \u2192 Grasp bottle \u2192 Navigate to bin \u2192 Place bottle\r\n   \u2192 Monitor each step \u2192 Report completion\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-safety-boundary-implementation",children:"Example: Safety Boundary Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Component: Navigation Module\r\nInput: Target coordinates (x, y)\r\nOutput: Navigation success/failure\r\n\r\nSafety Checks:\r\n1. Collision Avoidance: Check path for obstacles every 0.5 seconds\r\n2. Boundary Limits: Ensure target within safe operational area\r\n3. Time Limits: Abort if navigation takes > 3x estimated time\r\n4. Human Detection: Stop if human enters safety zone (radius 1m)\r\n\r\nIf any safety check fails:\r\n\u2192 Emergency stop\r\n\u2192 Report failure to monitoring system\r\n\u2192 Request re-plan with safety considerations\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary--key-takeaways",children:"Summary & Key Takeaways"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, you learned about system architecture for the autonomous humanoid system:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception \u2192 Planning \u2192 Action loop"})," creates a continuous cycle connecting all system components"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Component isolation"})," provides fault tolerance and maintainability through clear boundaries"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deterministic vs probabilistic modules"})," require different integration strategies and safety considerations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety boundaries"})," are integrated throughout the architecture to ensure reliable operation"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This architectural foundation enables the integration of voice, vision, and action components while maintaining safety and observability. The next chapters will build on this foundation to implement the complete capstone system that connects all previous modules into a cohesive autonomous humanoid system."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);