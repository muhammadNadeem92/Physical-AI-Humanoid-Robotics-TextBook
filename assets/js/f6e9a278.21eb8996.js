"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[257],{8453(e,n,r){r.d(n,{R:()=>o,x:()=>t});var a=r(6540);const s={},i=a.createContext(s);function o(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(i.Provider,{value:n},e.children)}},9415(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-05-isaac-ai-brain/perception-navigation","title":"Chapter 3: Perception, Localization & Navigation","description":"Introduction","source":"@site/docs/module-05-isaac-ai-brain/perception-navigation.md","sourceDirName":"module-05-isaac-ai-brain","slug":"/module-05-isaac-ai-brain/perception-navigation","permalink":"/docs/module-05-isaac-ai-brain/perception-navigation","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadNadeem92/physical-ai-humanoid-robotics-textbook/tree/main/docs/module-05-isaac-ai-brain/perception-navigation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Photorealistic Simulation & Synthetic Data","permalink":"/docs/module-05-isaac-ai-brain/synthetic-data"},"next":{"title":"Chapter 4: Learning, Sim-to-Real & Performance","permalink":"/docs/module-05-isaac-ai-brain/learning-sim2real"}}');var s=r(4848),i=r(8453);const o={},t="Chapter 3: Perception, Localization & Navigation",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Isaac ROS Packages",id:"isaac-ros-packages",level:3},{value:"Visual SLAM (VSLAM)",id:"visual-slam-vslam",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:3},{value:"Nav2 Path Planning for Humanoid Proxies",id:"nav2-path-planning-for-humanoid-proxies",level:3},{value:"Examples",id:"examples",level:2},{value:"Example: Run Isaac ROS VSLAM",id:"example-run-isaac-ros-vslam",level:3},{value:"Example: Publish Localization Data to ROS 2",id:"example-publish-localization-data-to-ros-2",level:3},{value:"Example: Navigate a Robot Through Obstacles",id:"example-navigate-a-robot-through-obstacles",level:3},{value:"Summary &amp; Key Takeaways",id:"summary--key-takeaways",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-3-perception-localization--navigation",children:"Chapter 3: Perception, Localization & Navigation"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"In the previous chapter, you learned about photorealistic simulation and synthetic data generation using Isaac Sim. Now we'll explore how to implement perception, localization, and navigation using GPU-accelerated pipelines in Isaac ROS. This chapter enables autonomous movement using Isaac's GPU-accelerated capabilities, building on the simulation foundation to create intelligent robot behaviors."}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS transforms traditional ROS 2 nodes by leveraging NVIDIA's GPU acceleration for perception and navigation tasks. Unlike CPU-based processing, Isaac ROS nodes can handle high-resolution sensor data in real-time, enabling complex perception and navigation capabilities that are essential for autonomous humanoid robots."}),"\n",(0,s.jsx)(n.p,{children:"The integration of Visual SLAM (VSLAM) with Isaac ROS provides robust localization capabilities that allow robots to understand their position and environment using only camera inputs. Combined with Nav2 path planning, this creates a complete autonomous navigation system that can operate in complex environments."}),"\n",(0,s.jsx)(n.p,{children:"This chapter covers Isaac ROS packages, Visual SLAM implementation, sensor fusion techniques, and Nav2 path planning specifically adapted for humanoid robot proxies. You'll learn how to run Isaac ROS VSLAM, publish localization data to ROS 2, and navigate robots through obstacles using GPU-accelerated processing."}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you'll be able to deploy Isaac ROS perception nodes, implement Visual SLAM for localization, and create navigation systems that enable autonomous robot movement through complex environments."}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-packages",children:"Isaac ROS Packages"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS packages provide GPU-accelerated versions of common robotics algorithms and include:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Apriltag"}),": GPU-accelerated AprilTag detection for precise fiducial marker localization. This package can process high-resolution images in real-time, detecting and decoding AprilTags with high accuracy and low latency."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Stereo DNN"}),": GPU-accelerated deep neural network inference for stereo vision tasks. This package enables real-time semantic segmentation, object detection, and other perception tasks using stereo camera inputs."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Visual Slam"}),": GPU-accelerated visual SLAM algorithms that provide real-time localization and mapping using camera inputs. These packages significantly outperform CPU-based SLAM implementations in terms of speed and accuracy."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Image Pipeline"}),": GPU-accelerated image processing pipeline that includes rectification, resizing, and format conversion. This pipeline enables efficient preprocessing of camera data before it's processed by perception algorithms."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Point Cloud"}),": GPU-accelerated point cloud processing for LiDAR and stereo vision applications. This package can generate, process, and filter point clouds in real-time using GPU acceleration."]}),"\n",(0,s.jsx)(n.h3,{id:"visual-slam-vslam",children:"Visual SLAM (VSLAM)"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) in Isaac ROS provides:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-Time Processing"}),": GPU acceleration enables real-time SLAM processing that can handle high-resolution camera feeds with minimal latency."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Robust Tracking"}),": Advanced tracking algorithms that maintain pose estimation even in challenging conditions with fast motion or repetitive patterns."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Map Building"}),": Construction of 3D maps from visual inputs that can be used for navigation and path planning."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Loop Closure"}),": Detection of previously visited locations to correct drift in the estimated trajectory."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-Resolution Processing"}),": Efficient processing using multiple image resolutions to balance accuracy and performance."]}),"\n",(0,s.jsx)(n.h3,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS implements sophisticated sensor fusion techniques that combine multiple sensor inputs:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Visual-Inertial Fusion"}),": Combining camera and IMU data to improve pose estimation accuracy and robustness. The IMU provides high-frequency motion data that complements the visual tracking."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-Sensor Integration"}),": Combining data from cameras, LiDAR, IMU, and other sensors to create robust perception systems that work in various conditions."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Kalman Filtering"}),": Advanced filtering techniques that optimally combine sensor measurements to produce accurate state estimates."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Uncertainty Management"}),": Proper handling of sensor uncertainties and correlations to produce reliable state estimates."]}),"\n",(0,s.jsx)(n.h3,{id:"nav2-path-planning-for-humanoid-proxies",children:"Nav2 Path Planning for Humanoid Proxies"}),"\n",(0,s.jsx)(n.p,{children:"Nav2 (Navigation 2) in Isaac ROS is adapted for humanoid robots with:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Humanoid-Aware Planning"}),": Path planning algorithms that consider the specific kinematic and dynamic constraints of humanoid robots."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"3D Navigation"}),": Support for navigation in 3D environments that considers the full pose of the humanoid robot."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Dynamic Obstacle Avoidance"}),": Real-time obstacle detection and avoidance that accounts for moving obstacles in the environment."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Social Navigation"}),": Consideration of human-aware navigation that respects personal space and social norms."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Legged Locomotion Integration"}),": Coordination between navigation planning and legged locomotion controllers for stable movement."]}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.h3,{id:"example-run-isaac-ros-vslam",children:"Example: Run Isaac ROS VSLAM"}),"\n",(0,s.jsx)(n.p,{children:"Setting up and running Isaac ROS Visual SLAM for real-time localization:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# vslam_pipeline_config.yaml\r\ncamera:\r\n  width: 1920\r\n  height: 1080\r\n  fps: 30\r\n  distortion_model: "rational_polynomial"\r\n  distortion_coefficients: [-0.12, 0.15, 0.0005, -0.0005, 0.0]\r\n  camera_matrix: [600.0, 0.0, 960.0, 0.0, 600.0, 540.0, 0.0, 0.0, 1.0]\r\n\r\ntracking:\r\n  max_features: 2000\r\n  min_distance: 15\r\n  quality_level: 0.01\r\n  pyramid_levels: 3\r\n  patch_size: 21\r\n\r\nmapping:\r\n  keyframe_overlap: 0.7\r\n  max_keyframes: 200\r\n  min_triangulation_angle: 10\r\n  bundle_adjustment_frequency: 10\r\n\r\nloop_closure:\r\n  detection_frequency: 5\r\n  min_matches: 15\r\n  similarity_threshold: 0.7\r\n  geometric_verification: true\r\n\r\ngpu_settings:\r\n  use_tensor_cores: true\r\n  memory_budget_mb: 4096\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# vslam_example.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav_msgs.msg import Odometry\r\nfrom visualization_msgs.msg import MarkerArray\r\nimport numpy as np\r\n\r\nclass IsaacVSLAMNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_vslam_node\')\r\n\r\n        # Publishers and subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.odom_pub = self.create_publisher(\r\n            Odometry,\r\n            \'/visual_slam/odometry\',\r\n            10\r\n        )\r\n\r\n        self.pose_pub = self.create_publisher(\r\n            PoseStamped,\r\n            \'/visual_slam/pose\',\r\n            10\r\n        )\r\n\r\n        self.map_pub = self.create_publisher(\r\n            MarkerArray,\r\n            \'/visual_slam/map\',\r\n            10\r\n        )\r\n\r\n        # Initialize Isaac VSLAM components\r\n        self.initialize_vslam()\r\n\r\n        # Timer for periodic processing\r\n        self.timer = self.create_timer(0.033, self.process_vslam)  # ~30 Hz\r\n\r\n        self.get_logger().info(\'Isaac VSLAM node initialized\')\r\n\r\n    def initialize_vslam(self):\r\n        """Initialize GPU-accelerated VSLAM components"""\r\n        # This would typically interface with Isaac ROS VSLAM packages\r\n        # For demonstration, we\'ll set up placeholder components\r\n        self.vslam_initialized = True\r\n        self.current_pose = np.eye(4)  # 4x4 transformation matrix\r\n        self.keyframes = []\r\n        self.feature_points = []\r\n\r\n        self.get_logger().info(\'VSLAM components initialized\')\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming camera images"""\r\n        if not self.vslam_initialized:\r\n            return\r\n\r\n        # Convert ROS image to format suitable for GPU processing\r\n        # In a real implementation, this would use Isaac ROS image pipeline\r\n        image_data = self.convert_ros_image_to_gpu_format(msg)\r\n\r\n        # Process the image with GPU-accelerated VSLAM\r\n        self.process_image_with_vslam(image_data)\r\n\r\n    def process_image_with_vslam(self, image_data):\r\n        """Process image using GPU-accelerated VSLAM"""\r\n        # This would interface with actual Isaac ROS VSLAM packages\r\n        # For demonstration, we\'ll simulate the processing\r\n        if len(self.keyframes) == 0:\r\n            # First frame - create initial keyframe\r\n            self.keyframes.append({\r\n                \'pose\': self.current_pose.copy(),\r\n                \'image\': image_data,\r\n                \'timestamp\': self.get_clock().now().nanoseconds\r\n            })\r\n        else:\r\n            # Track features and estimate motion\r\n            motion_estimate = self.estimate_motion_gpu(image_data)\r\n            if motion_estimate is not None:\r\n                # Update current pose\r\n                self.current_pose = np.dot(self.current_pose, motion_estimate)\r\n\r\n                # Check if we should create a new keyframe\r\n                if self.should_create_keyframe():\r\n                    self.keyframes.append({\r\n                        \'pose\': self.current_pose.copy(),\r\n                        \'image\': image_data,\r\n                        \'timestamp\': self.get_clock().now().nanoseconds\r\n                    })\r\n\r\n    def estimate_motion_gpu(self, image_data):\r\n        """Estimate motion using GPU-accelerated feature tracking"""\r\n        # Placeholder for actual GPU-accelerated motion estimation\r\n        # In a real implementation, this would use Isaac ROS tracking packages\r\n        # Simulate small random motion for demonstration\r\n        dt = 0.033  # 30 Hz\r\n        linear_vel = np.random.uniform(-0.1, 0.1, 3)  # m/s\r\n        angular_vel = np.random.uniform(-0.1, 0.1, 3)  # rad/s\r\n\r\n        # Convert velocities to transformation matrix\r\n        motion = np.eye(4)\r\n        motion[0:3, 3] = linear_vel * dt\r\n\r\n        # Simple rotation approximation\r\n        angle = np.linalg.norm(angular_vel) * dt\r\n        if angle > 1e-6:\r\n            axis = angular_vel / np.linalg.norm(angular_vel)\r\n            motion[0:3, 0:3] = self.axis_angle_to_rotation_matrix(axis, angle)\r\n\r\n        return motion\r\n\r\n    def should_create_keyframe(self):\r\n        """Determine if a new keyframe should be created"""\r\n        if len(self.keyframes) == 0:\r\n            return True\r\n\r\n        # Check if the camera has moved significantly\r\n        last_pose = self.keyframes[-1][\'pose\']\r\n        translation = np.linalg.norm(self.current_pose[0:3, 3] - last_pose[0:3, 3])\r\n        rotation = np.arccos(np.clip((np.trace(self.current_pose[0:3, 0:3].T @ last_pose[0:3, 0:3]) - 1) / 2, -1, 1))\r\n\r\n        # Create keyframe if translation > 0.5m or rotation > 10 degrees\r\n        return translation > 0.5 or rotation > np.deg2rad(10)\r\n\r\n    def process_vslam(self):\r\n        """Periodic VSLAM processing"""\r\n        if not self.vslam_initialized:\r\n            return\r\n\r\n        # Publish current pose\r\n        pose_msg = PoseStamped()\r\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\r\n        pose_msg.header.frame_id = \'map\'\r\n        pose_msg.pose.position.x = self.current_pose[0, 3]\r\n        pose_msg.pose.position.y = self.current_pose[1, 3]\r\n        pose_msg.pose.position.z = self.current_pose[2, 3]\r\n\r\n        # Convert rotation matrix to quaternion\r\n        quat = self.rotation_matrix_to_quaternion(self.current_pose[0:3, 0:3])\r\n        pose_msg.pose.orientation.x = quat[0]\r\n        pose_msg.pose.orientation.y = quat[1]\r\n        pose_msg.pose.orientation.z = quat[2]\r\n        pose_msg.pose.orientation.w = quat[3]\r\n\r\n        self.pose_pub.publish(pose_msg)\r\n\r\n        # Publish odometry\r\n        odom_msg = Odometry()\r\n        odom_msg.header.stamp = pose_msg.header.stamp\r\n        odom_msg.header.frame_id = \'map\'\r\n        odom_msg.child_frame_id = \'base_link\'\r\n        odom_msg.pose.pose = pose_msg.pose\r\n\r\n        self.odom_pub.publish(odom_msg)\r\n\r\n    def convert_ros_image_to_gpu_format(self, image_msg):\r\n        """Convert ROS image message to GPU-compatible format"""\r\n        # Placeholder for actual conversion\r\n        return np.random.rand(image_msg.height, image_msg.width, 3)  # Simulated image\r\n\r\n    def axis_angle_to_rotation_matrix(self, axis, angle):\r\n        """Convert axis-angle representation to rotation matrix"""\r\n        # Rodrigues\' rotation formula\r\n        k = axis\r\n        K = np.array([[0, -k[2], k[1]], [k[2], 0, -k[0]], [-k[1], k[0], 0]])\r\n        R = np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * np.dot(K, K)\r\n        return R\r\n\r\n    def rotation_matrix_to_quaternion(self, R):\r\n        """Convert rotation matrix to quaternion"""\r\n        # Algorithm from https://www.euclideanspace.com/maths/geometry/rotations/conversions/matrixToQuaternion/\r\n        trace = np.trace(R)\r\n        if trace > 0:\r\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw\r\n            qw = 0.25 * s\r\n            qx = (R[2, 1] - R[1, 2]) / s\r\n            qy = (R[0, 2] - R[2, 0]) / s\r\n            qz = (R[1, 0] - R[0, 1]) / s\r\n        else:\r\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\r\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\r\n                qw = (R[2, 1] - R[1, 2]) / s\r\n                qx = 0.25 * s\r\n                qy = (R[0, 1] + R[1, 0]) / s\r\n                qz = (R[0, 2] + R[2, 0]) / s\r\n            elif R[1, 1] > R[2, 2]:\r\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\r\n                qw = (R[0, 2] - R[2, 0]) / s\r\n                qx = (R[0, 1] + R[1, 0]) / s\r\n                qy = 0.25 * s\r\n                qz = (R[1, 2] + R[2, 1]) / s\r\n            else:\r\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\r\n                qw = (R[1, 0] - R[0, 1]) / s\r\n                qx = (R[0, 2] + R[2, 0]) / s\r\n                qy = (R[1, 2] + R[2, 1]) / s\r\n                qz = 0.25 * s\r\n\r\n        return np.array([qx, qy, qz, qw])\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = IsaacVSLAMNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-publish-localization-data-to-ros-2",children:"Example: Publish Localization Data to ROS 2"}),"\n",(0,s.jsx)(n.p,{children:"Publishing localization data from Isaac ROS VSLAM to the ROS 2 ecosystem:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# localization_publisher.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\r\nfrom sensor_msgs.msg import Imu\r\nfrom tf2_ros import TransformBroadcaster\r\nfrom geometry_msgs.msg import TransformStamped\r\nimport tf_transformations\r\nimport numpy as np\r\n\r\nclass LocalizationPublisher(Node):\r\n    def __init__(self):\r\n        super().__init__('localization_publisher')\r\n\r\n        # Publisher for initial pose estimation\r\n        self.initial_pose_pub = self.create_publisher(\r\n            PoseWithCovarianceStamped,\r\n            '/initialpose',\r\n            10\r\n        )\r\n\r\n        # Publisher for AMCL pose\r\n        self.amcl_pose_pub = self.create_publisher(\r\n            PoseWithCovarianceStamped,\r\n            '/amcl_pose',\r\n            10\r\n        )\r\n\r\n        # Initialize TF broadcaster\r\n        self.tf_broadcaster = TransformBroadcaster(self)\r\n\r\n        # Subscribe to VSLAM pose\r\n        self.vslam_pose_sub = self.create_subscription(\r\n            PoseStamped,\r\n            '/visual_slam/pose',\r\n            self.vslam_pose_callback,\r\n            10\r\n        )\r\n\r\n        # Store the last known pose for TF publishing\r\n        self.last_pose = None\r\n\r\n        self.get_logger().info('Localization publisher initialized')\r\n\r\n    def vslam_pose_callback(self, msg):\r\n        \"\"\"Handle incoming VSLAM pose data\"\"\"\r\n        # Convert VSLAM pose to various ROS 2 message types\r\n\r\n        # Publish AMCL pose (for Nav2 compatibility)\r\n        amcl_pose = PoseWithCovarianceStamped()\r\n        amcl_pose.header = msg.header\r\n        amcl_pose.pose.pose = msg.pose\r\n\r\n        # Set covariance (these values should be estimated by VSLAM)\r\n        covariance = np.array([\r\n            0.1, 0.0, 0.0, 0.0, 0.0, 0.0,  # x, y, z\r\n            0.0, 0.1, 0.0, 0.0, 0.0, 0.0,  # rx, ry, rz\r\n            0.0, 0.0, 0.1, 0.0, 0.0, 0.0,  # others\r\n            0.0, 0.0, 0.0, 0.1, 0.0, 0.0,\r\n            0.0, 0.0, 0.0, 0.0, 0.1, 0.0,\r\n            0.0, 0.0, 0.0, 0.0, 0.0, 0.1\r\n        ])\r\n        amcl_pose.pose.covariance = covariance.flatten().tolist()\r\n\r\n        self.amcl_pose_pub.publish(amcl_pose)\r\n\r\n        # Store for TF publishing\r\n        self.last_pose = msg\r\n\r\n        # Publish TF transform\r\n        self.publish_transform(msg)\r\n\r\n    def publish_transform(self, pose_msg):\r\n        \"\"\"Publish TF transform from map to base_link\"\"\"\r\n        t = TransformStamped()\r\n\r\n        # Set header\r\n        t.header.stamp = pose_msg.header.stamp\r\n        t.header.frame_id = 'map'\r\n        t.child_frame_id = 'base_link'\r\n\r\n        # Set transform values\r\n        t.transform.translation.x = pose_msg.pose.position.x\r\n        t.transform.translation.y = pose_msg.pose.position.y\r\n        t.transform.translation.z = pose_msg.pose.position.z\r\n        t.transform.rotation = pose_msg.pose.orientation\r\n\r\n        # Send transform\r\n        self.tf_broadcaster.sendTransform(t)\r\n\r\n        # Also publish base_link to camera_link transform (if applicable)\r\n        cam_t = TransformStamped()\r\n        cam_t.header.stamp = pose_msg.header.stamp\r\n        cam_t.header.frame_id = 'base_link'\r\n        cam_t.child_frame_id = 'camera_link'\r\n\r\n        # Camera offset from base (example: 0.1m forward, 0.5m up)\r\n        cam_t.transform.translation.x = 0.1\r\n        cam_t.transform.translation.y = 0.0\r\n        cam_t.transform.translation.z = 0.5\r\n        cam_t.transform.rotation.w = 1.0  # No rotation\r\n\r\n        self.tf_broadcaster.sendTransform(cam_t)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = LocalizationPublisher()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"example-navigate-a-robot-through-obstacles",children:"Example: Navigate a Robot Through Obstacles"}),"\n",(0,s.jsx)(n.p,{children:"Implementing navigation using Isaac ROS with Nav2 for obstacle avoidance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# navigation_example.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom action_msgs.msg import GoalStatus\r\nfrom builtin_interfaces.msg import Duration\r\nimport time\r\n\r\nclass IsaacNavigationNode(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_navigation_node')\r\n\r\n        # Create action client for navigation\r\n        self.nav_to_pose_client = ActionClient(\r\n            self, NavigateToPose, 'navigate_to_pose'\r\n        )\r\n\r\n        # Publisher for sending navigation goals\r\n        self.goal_publisher = self.create_publisher(\r\n            PoseStamped,\r\n            '/goal_pose',\r\n            10\r\n        )\r\n\r\n        # Timer to send navigation goals periodically\r\n        self.nav_timer = self.create_timer(10.0, self.send_navigation_goal)\r\n\r\n        self.get_logger().info('Isaac Navigation node initialized')\r\n\r\n    def send_navigation_goal(self):\r\n        \"\"\"Send a navigation goal to Nav2\"\"\"\r\n        goal_msg = NavigateToPose.Goal()\r\n\r\n        # Set the goal pose\r\n        goal_msg.pose.header.frame_id = 'map'\r\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\r\n        goal_msg.pose.pose.position.x = 5.0  # Example goal position\r\n        goal_msg.pose.pose.position.y = 3.0\r\n        goal_msg.pose.pose.position.z = 0.0\r\n\r\n        # Set orientation (facing forward)\r\n        goal_msg.pose.pose.orientation.w = 1.0\r\n\r\n        # Set navigation options\r\n        goal_msg.behavior_tree = ''  # Use default behavior tree\r\n\r\n        # Send the goal\r\n        self.nav_to_pose_client.wait_for_server()\r\n        future = self.nav_to_pose_client.send_goal_async(\r\n            goal_msg,\r\n            feedback_callback=self.feedback_callback\r\n        )\r\n\r\n        future.add_done_callback(self.goal_response_callback)\r\n\r\n    def goal_response_callback(self, future):\r\n        \"\"\"Handle the goal response\"\"\"\r\n        goal_handle = future.result()\r\n        if not goal_handle.accepted:\r\n            self.get_logger().info('Goal rejected')\r\n            return\r\n\r\n        self.get_logger().info('Goal accepted')\r\n\r\n        # Get result future\r\n        result_future = goal_handle.get_result_async()\r\n        result_future.add_done_callback(self.get_result_callback)\r\n\r\n    def get_result_callback(self, future):\r\n        \"\"\"Handle the navigation result\"\"\"\r\n        status = future.result().status\r\n        if status == GoalStatus.STATUS_SUCCEEDED:\r\n            self.get_logger().info('Navigation succeeded')\r\n        else:\r\n            self.get_logger().info(f'Navigation failed with status: {status}')\r\n\r\n    def feedback_callback(self, feedback_msg):\r\n        \"\"\"Handle navigation feedback\"\"\"\r\n        feedback = feedback_msg.feedback\r\n        remaining_distance = feedback.distance_remaining\r\n        self.get_logger().info(f'Distance remaining: {remaining_distance:.2f}m')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = IsaacNavigationNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary--key-takeaways",children:"Summary & Key Takeaways"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, you've learned about perception, localization, and navigation using GPU-accelerated pipelines in Isaac ROS:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS packages"})," provide GPU-accelerated versions of common robotics algorithms including Apriltag detection, stereo DNN, visual SLAM, and image processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual SLAM (VSLAM)"})," enables real-time localization and mapping using camera inputs with GPU acceleration for improved performance and accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor fusion"})," techniques combine multiple sensor inputs to create robust perception systems that work in various conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Nav2 path planning"})," is adapted for humanoid robots with awareness of their specific kinematic and dynamic constraints"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"You've seen practical examples of running Isaac ROS VSLAM with configuration files and code, publishing localization data to the ROS 2 ecosystem, and implementing navigation through obstacles using Nav2. These capabilities enable autonomous robot movement and intelligent behavior in complex environments."}),"\n",(0,s.jsx)(n.p,{children:"The GPU acceleration provided by Isaac ROS is crucial for real-time processing of high-resolution sensor data, which is essential for humanoid robots that need to perceive and navigate in real-world environments. The integration with standard ROS 2 interfaces ensures compatibility with existing robotics tools and workflows."}),"\n",(0,s.jsx)(n.p,{children:"In the next chapter, we'll explore learning-based control and performance considerations that prepare you for real-world deployment of Isaac-based robotic systems."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}}}]);