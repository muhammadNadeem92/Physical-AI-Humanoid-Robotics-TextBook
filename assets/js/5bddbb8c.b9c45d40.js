"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[573],{4814(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-04-digital-twin/unity-hri","title":"Chapter 3: Unity for Visualization & Human\u2013Robot Interaction","description":"Introduction","source":"@site/docs/module-04-digital-twin/unity-hri.md","sourceDirName":"module-04-digital-twin","slug":"/module-04-digital-twin/unity-hri","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-04-digital-twin/unity-hri","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadNadeem92/Physical-AI-Humanoid-Robotics-TextBook/tree/main/docs/module-04-digital-twin/unity-hri.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Gazebo Simulation with ROS 2","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-04-digital-twin/gazebo-ros2"},"next":{"title":"Chapter 4: Sim-to-Real Strategy & Validation","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-04-digital-twin/sim-to-real"}}');var i=t(4848),o=t(8453);const a={},s="Chapter 3: Unity for Visualization & Human\u2013Robot Interaction",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Why Unity is Used in Robotics",id:"why-unity-is-used-in-robotics",level:3},{value:"Graphics vs Physics Accuracy Trade-offs",id:"graphics-vs-physics-accuracy-trade-offs",level:3},{value:"ROS\u2013Unity Communication Approaches",id:"rosunity-communication-approaches",level:3},{value:"Human\u2013Robot Interaction (HRI) in Unity",id:"humanrobot-interaction-hri-in-unity",level:3},{value:"Examples",id:"examples",level:2},{value:"Example: Unity Scene with Robot Avatar",id:"example-unity-scene-with-robot-avatar",level:3},{value:"Example: Keyboard or Gesture-Based Robot Control",id:"example-keyboard-or-gesture-based-robot-control",level:3},{value:"Example: Visualization-Only Digital Twin",id:"example-visualization-only-digital-twin",level:3},{value:"Summary &amp; Key Takeaways",id:"summary--key-takeaways",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-3-unity-for-visualization--humanrobot-interaction",children:"Chapter 3: Unity for Visualization & Human\u2013Robot Interaction"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"In the previous chapters, you learned about digital twin concepts and how to implement physics-based simulation using Gazebo with ROS 2 integration. Now we'll explore Unity, a powerful visualization platform that excels in creating engaging, realistic environments for human-robot interaction (HRI) scenarios."}),"\n",(0,i.jsx)(e.p,{children:"While Gazebo focuses on physics accuracy and realistic simulation of robot behaviors, Unity specializes in high-quality graphics, immersive environments, and intuitive human-robot interaction design. Unity provides an excellent complement to physics-focused simulators by offering advanced visualization capabilities and user-friendly interfaces for testing interaction scenarios."}),"\n",(0,i.jsx)(e.p,{children:"Unity's strength lies in its ability to create photorealistic environments and intuitive interfaces that help researchers and developers visualize robot behaviors in contexts that closely resemble real-world deployment scenarios. This is particularly valuable for humanoid robots, which are designed to operate in human environments and interact with humans."}),"\n",(0,i.jsx)(e.p,{children:"This chapter explores why Unity is used in robotics, the trade-offs between graphics and physics accuracy, ROS-Unity communication approaches, and how to design effective human-robot interaction scenarios. You'll learn how Unity can serve as a visualization-only digital twin or be integrated with physics engines for more comprehensive simulation."}),"\n",(0,i.jsx)(e.p,{children:"By the end of this chapter, you'll understand when to use Unity for robotics applications, how to create engaging visualization environments, and how to implement human-robot interaction scenarios that complement physics-based simulation approaches."}),"\n",(0,i.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,i.jsx)(e.h3,{id:"why-unity-is-used-in-robotics",children:"Why Unity is Used in Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Unity has become increasingly popular in robotics for several key reasons:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"High-Quality Graphics"}),": Unity's rendering pipeline produces photorealistic visuals that help researchers visualize robot behaviors in realistic environments. This is crucial for testing perception algorithms and creating compelling demonstrations."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Asset Ecosystem"}),": Unity's Asset Store provides thousands of pre-built models, environments, and tools that can accelerate robotics development. This includes furniture, buildings, robots, and specialized robotics tools."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Cross-Platform Deployment"}),": Unity allows deployment to multiple platforms including desktop, mobile, VR, and AR, enabling diverse testing and interaction scenarios."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"User Interface Design"}),": Unity's UI system makes it easy to create intuitive interfaces for robot teleoperation, monitoring, and control."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Real-Time Performance"}),": Unity is optimized for real-time rendering, making it suitable for interactive applications and real-time visualization of robot data."]}),"\n",(0,i.jsx)(e.h3,{id:"graphics-vs-physics-accuracy-trade-offs",children:"Graphics vs Physics Accuracy Trade-offs"}),"\n",(0,i.jsx)(e.p,{children:"When choosing between Unity and physics-focused simulators like Gazebo, consider these important trade-offs:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Unity Strengths"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Superior visual quality and rendering"}),"\n",(0,i.jsx)(e.li,{children:"Better user interface and interaction design capabilities"}),"\n",(0,i.jsx)(e.li,{children:"Faster rendering of complex scenes"}),"\n",(0,i.jsx)(e.li,{children:"More intuitive environment creation tools"}),"\n",(0,i.jsx)(e.li,{children:"Better integration with VR/AR platforms"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Unity Limitations"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Less accurate physics simulation compared to dedicated physics engines"}),"\n",(0,i.jsx)(e.li,{children:"More complex ROS integration requiring specialized middleware"}),"\n",(0,i.jsx)(e.li,{children:"Higher computational requirements for high-quality graphics"}),"\n",(0,i.jsx)(e.li,{children:"Steeper learning curve for robotics-specific applications"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"The choice between Unity and Gazebo often depends on the specific requirements of your application:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use Gazebo for physics-accurate simulation and control algorithm testing"}),"\n",(0,i.jsx)(e.li,{children:"Use Unity for visualization, perception testing, and human-robot interaction"}),"\n",(0,i.jsx)(e.li,{children:"Use both in combination for comprehensive digital twin environments"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"rosunity-communication-approaches",children:"ROS\u2013Unity Communication Approaches"}),"\n",(0,i.jsx)(e.p,{children:"Connecting Unity with ROS 2 requires specialized middleware that bridges the two systems:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Unity Robotics Hub"}),": Provides official packages for ROS-Unity integration, including:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"ROS TCP Connector: Enables communication between ROS and Unity via TCP/IP"}),"\n",(0,i.jsx)(e.li,{children:"URDF Importer: Allows importing robot models directly from URDF files"}),"\n",(0,i.jsx)(e.li,{children:"Robotics Demo Framework: Example implementations of common robotics patterns"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Message Translation"}),": Converting between ROS message types and Unity data structures, including:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Transform synchronization between ROS TF tree and Unity coordinate system"}),"\n",(0,i.jsx)(e.li,{children:"Sensor data translation from ROS sensor_msgs to Unity representations"}),"\n",(0,i.jsx)(e.li,{children:"Control command translation from Unity to ROS joint commands"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Communication Patterns"}),": Implementing publisher-subscriber and service-client patterns between ROS and Unity systems."]}),"\n",(0,i.jsx)(e.h3,{id:"humanrobot-interaction-hri-in-unity",children:"Human\u2013Robot Interaction (HRI) in Unity"}),"\n",(0,i.jsx)(e.p,{children:"Unity excels at creating intuitive interfaces for human-robot interaction:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Teleoperation Interfaces"}),": Creating user-friendly controls for remote robot operation, including:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Joystick and keyboard interfaces"}),"\n",(0,i.jsx)(e.li,{children:"VR/AR teleoperation systems"}),"\n",(0,i.jsx)(e.li,{children:"Gesture-based control interfaces"}),"\n",(0,i.jsx)(e.li,{children:"Touch-screen interfaces for mobile platforms"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Visualization Systems"}),": Displaying robot state, sensor data, and environmental information in intuitive ways:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"3D visualization of sensor data (point clouds, camera feeds, etc.)"}),"\n",(0,i.jsx)(e.li,{children:"Robot state displays (battery, joint angles, system status)"}),"\n",(0,i.jsx)(e.li,{children:"Path planning visualization"}),"\n",(0,i.jsx)(e.li,{children:"Collision detection and safety zones"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Interaction Design"}),": Creating natural ways for humans to interact with robots:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Voice command interfaces"}),"\n",(0,i.jsx)(e.li,{children:"Gesture recognition systems"}),"\n",(0,i.jsx)(e.li,{children:"Social robot interaction scenarios"}),"\n",(0,i.jsx)(e.li,{children:"Collaborative task interfaces"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsx)(e.h3,{id:"example-unity-scene-with-robot-avatar",children:"Example: Unity Scene with Robot Avatar"}),"\n",(0,i.jsx)(e.p,{children:"Creating a basic Unity scene with a robot avatar involves several key components:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-csharp",children:'// RobotAvatarController.cs\r\nusing UnityEngine;\r\nusing System.Collections;\r\n\r\npublic class RobotAvatarController : MonoBehaviour\r\n{\r\n    [Header("Robot Configuration")]\r\n    public string robotName = "HumanoidRobot";\r\n    public float movementSpeed = 2.0f;\r\n    public float rotationSpeed = 100.0f;\r\n\r\n    [Header("Joint References")]\r\n    public Transform head;\r\n    public Transform leftArm;\r\n    public Transform rightArm;\r\n    public Transform leftLeg;\r\n    public Transform rightLeg;\r\n\r\n    [Header("ROS Connection")]\r\n    public bool isConnected = false;\r\n    public string rosEndpoint = "ws://localhost:9090";\r\n\r\n    private Vector3 targetPosition;\r\n    private Quaternion targetRotation;\r\n    private bool useROSMovement = false;\r\n\r\n    void Start()\r\n    {\r\n        targetPosition = transform.position;\r\n        targetRotation = transform.rotation;\r\n        InitializeROSConnection();\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        if (useROSMovement)\r\n        {\r\n            // Move based on ROS commands\r\n            UpdateFromROS();\r\n        }\r\n        else\r\n        {\r\n            // Move based on keyboard input for demonstration\r\n            HandleKeyboardInput();\r\n        }\r\n\r\n        // Smoothly interpolate to target position and rotation\r\n        transform.position = Vector3.Lerp(transform.position, targetPosition, Time.deltaTime * movementSpeed);\r\n        transform.rotation = Quaternion.Slerp(transform.rotation, targetRotation, Time.deltaTime * rotationSpeed);\r\n    }\r\n\r\n    void HandleKeyboardInput()\r\n    {\r\n        float horizontal = Input.GetAxis("Horizontal");\r\n        float vertical = Input.GetAxis("Vertical");\r\n\r\n        Vector3 movement = new Vector3(horizontal, 0, vertical) * movementSpeed * Time.deltaTime;\r\n        targetPosition += transform.TransformDirection(movement);\r\n\r\n        if (Input.GetKey(KeyCode.Q))\r\n            targetRotation *= Quaternion.Euler(0, -rotationSpeed * Time.deltaTime, 0);\r\n        if (Input.GetKey(KeyCode.E))\r\n            targetRotation *= Quaternion.Euler(0, rotationSpeed * Time.deltaTime, 0);\r\n    }\r\n\r\n    void InitializeROSConnection()\r\n    {\r\n        // Placeholder for ROS connection initialization\r\n        // In practice, you\'d use Unity ROS TCP Connector or similar\r\n        Debug.Log($"Connecting to ROS at {rosEndpoint}");\r\n        isConnected = true;\r\n    }\r\n\r\n    void UpdateFromROS()\r\n    {\r\n        // This would be populated with actual ROS message data\r\n        // For demonstration, we\'ll just keep the current position\r\n    }\r\n\r\n    // Methods for updating joint positions based on ROS joint state messages\r\n    public void UpdateJointPositions(float[] jointPositions)\r\n    {\r\n        if (jointPositions.Length >= 5)\r\n        {\r\n            // Update joint rotations based on joint position array\r\n            if (head != null)\r\n                head.localRotation = Quaternion.Euler(0, 0, jointPositions[0] * Mathf.Rad2Deg);\r\n\r\n            if (leftArm != null)\r\n                leftArm.localRotation = Quaternion.Euler(jointPositions[1] * Mathf.Rad2Deg, 0, 0);\r\n\r\n            if (rightArm != null)\r\n                rightArm.localRotation = Quaternion.Euler(jointPositions[2] * Mathf.Rad2Deg, 0, 0);\r\n\r\n            if (leftLeg != null)\r\n                leftLeg.localRotation = Quaternion.Euler(jointPositions[3] * Mathf.Rad2Deg, 0, 0);\r\n\r\n            if (rightLeg != null)\r\n                rightLeg.localRotation = Quaternion.Euler(jointPositions[4] * Mathf.Rad2Deg, 0, 0);\r\n        }\r\n    }\r\n}\n'})}),"\n",(0,i.jsx)(e.h3,{id:"example-keyboard-or-gesture-based-robot-control",children:"Example: Keyboard or Gesture-Based Robot Control"}),"\n",(0,i.jsx)(e.p,{children:"Implementing intuitive control interfaces in Unity:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-csharp",children:'// HRIController.cs\r\nusing UnityEngine;\r\nusing UnityEngine.UI;\r\nusing System.Collections;\r\n\r\npublic class HRIController : MonoBehaviour\r\n{\r\n    [Header("Control Configuration")]\r\n    public RobotAvatarController robot;\r\n    public Text statusText;\r\n    public Slider speedSlider;\r\n    public Button[] actionButtons;\r\n\r\n    [Header("Gesture Recognition")]\r\n    public bool enableGestures = false;\r\n    public Camera gestureCamera;\r\n\r\n    void Start()\r\n    {\r\n        SetupUI();\r\n        SetupActionButtons();\r\n    }\r\n\r\n    void SetupUI()\r\n    {\r\n        if (speedSlider != null)\r\n        {\r\n            speedSlider.minValue = 0.5f;\r\n            speedSlider.maxValue = 5.0f;\r\n            speedSlider.value = 2.0f;\r\n            speedSlider.onValueChanged.AddListener(OnSpeedChanged);\r\n        }\r\n    }\r\n\r\n    void SetupActionButtons()\r\n    {\r\n        if (actionButtons != null)\r\n        {\r\n            for (int i = 0; i < actionButtons.Length; i++)\r\n            {\r\n                int index = i; // Capture for closure\r\n                actionButtons[i].onClick.AddListener(() => OnActionButtonClicked(index));\r\n            }\r\n        }\r\n    }\r\n\r\n    void OnSpeedChanged(float value)\r\n    {\r\n        if (robot != null)\r\n            robot.movementSpeed = value;\r\n\r\n        if (statusText != null)\r\n            statusText.text = $"Speed: {value:F1}x";\r\n    }\r\n\r\n    void OnActionButtonClicked(int actionIndex)\r\n    {\r\n        switch (actionIndex)\r\n        {\r\n            case 0: // Wave gesture\r\n                StartCoroutine(PerformWaveGesture());\r\n                break;\r\n            case 1: // Point gesture\r\n                StartCoroutine(PerformPointGesture());\r\n                break;\r\n            case 2: // Come here gesture\r\n                StartCoroutine(PerformComeGesture());\r\n                break;\r\n        }\r\n    }\r\n\r\n    IEnumerator PerformWaveGesture()\r\n    {\r\n        if (robot != null && robot.rightArm != null)\r\n        {\r\n            Quaternion originalRotation = robot.rightArm.localRotation;\r\n            Quaternion targetRotation = Quaternion.Euler(30, 0, 0);\r\n\r\n            for (float t = 0; t < 1; t += Time.deltaTime * 4)\r\n            {\r\n                robot.rightArm.localRotation = Quaternion.Slerp(originalRotation, targetRotation, t);\r\n                yield return null;\r\n            }\r\n\r\n            yield return new WaitForSeconds(0.5f);\r\n\r\n            for (float t = 0; t < 1; t += Time.deltaTime * 4)\r\n            {\r\n                robot.rightArm.localRotation = Quaternion.Slerp(targetRotation, originalRotation, t);\r\n                yield return null;\r\n            }\r\n        }\r\n    }\r\n\r\n    IEnumerator PerformPointGesture()\r\n    {\r\n        if (robot != null && robot.rightArm != null)\r\n        {\r\n            Quaternion originalRotation = robot.rightArm.localRotation;\r\n            Quaternion targetRotation = Quaternion.Euler(60, 30, 0);\r\n\r\n            for (float t = 0; t < 1; t += Time.deltaTime * 3)\r\n            {\r\n                robot.rightArm.localRotation = Quaternion.Slerp(originalRotation, targetRotation, t);\r\n                yield return null;\r\n            }\r\n\r\n            yield return new WaitForSeconds(1.0f);\r\n\r\n            for (float t = 0; t < 1; t += Time.deltaTime * 3)\r\n            {\r\n                robot.rightArm.localRotation = Quaternion.Slerp(targetRotation, originalRotation, t);\r\n                yield return null;\r\n            }\r\n        }\r\n    }\r\n\r\n    IEnumerator PerformComeGesture()\r\n    {\r\n        if (robot != null)\r\n        {\r\n            // Move robot toward a target position\r\n            Vector3 startPosition = robot.transform.position;\r\n            Vector3 targetPosition = startPosition + robot.transform.forward * 2.0f;\r\n\r\n            for (float t = 0; t < 1; t += Time.deltaTime * 0.5f)\r\n            {\r\n                robot.targetPosition = Vector3.Lerp(startPosition, targetPosition, t);\r\n                yield return null;\r\n            }\r\n        }\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        if (enableGestures && gestureCamera != null)\r\n        {\r\n            HandleGestureInput();\r\n        }\r\n    }\r\n\r\n    void HandleGestureInput()\r\n    {\r\n        // Simple gesture recognition based on mouse movement\r\n        if (Input.GetMouseButtonDown(0))\r\n        {\r\n            StartCoroutine(CaptureGesture());\r\n        }\r\n    }\r\n\r\n    IEnumerator CaptureGesture()\r\n    {\r\n        Vector3 gestureStart = Input.mousePosition;\r\n        yield return new WaitForSeconds(0.1f);\r\n        Vector3 gestureEnd = Input.mousePosition;\r\n\r\n        Vector2 gestureVector = (gestureEnd - gestureStart);\r\n\r\n        if (gestureVector.magnitude > 50) // Minimum gesture size\r\n        {\r\n            if (Mathf.Abs(gestureVector.x) > Mathf.Abs(gestureVector.y))\r\n            {\r\n                // Horizontal gesture - move left/right\r\n                if (gestureVector.x > 0)\r\n                    robot.targetPosition += robot.transform.right * 1.0f;\r\n                else\r\n                    robot.targetPosition -= robot.transform.right * 1.0f;\r\n            }\r\n            else\r\n            {\r\n                // Vertical gesture - move forward/back\r\n                if (gestureVector.y > 0)\r\n                    robot.targetPosition += robot.transform.forward * 1.0f;\r\n                else\r\n                    robot.targetPosition -= robot.transform.forward * 1.0f;\r\n            }\r\n        }\r\n    }\r\n}\n'})}),"\n",(0,i.jsx)(e.h3,{id:"example-visualization-only-digital-twin",children:"Example: Visualization-Only Digital Twin"}),"\n",(0,i.jsx)(e.p,{children:"Creating a visualization-only digital twin that displays real robot data:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-csharp",children:'// VisualizationTwin.cs\r\nusing UnityEngine;\r\nusing System.Collections.Generic;\r\nusing System.Linq;\r\n\r\npublic class VisualizationTwin : MonoBehaviour\r\n{\r\n    [Header("Twin Configuration")]\r\n    public RobotAvatarController robotAvatar;\r\n    public bool isLiveConnection = false;\r\n    public float updateRate = 30.0f; // Hz\r\n\r\n    [Header("Data Buffers")]\r\n    public int maxBufferSize = 100;\r\n    private Queue<Vector3> positionBuffer;\r\n    private Queue<Quaternion> rotationBuffer;\r\n    private Queue<float[]> jointStateBuffer;\r\n\r\n    [Header("Visualization Settings")]\r\n    public bool showTrajectory = true;\r\n    public GameObject trajectoryPrefab;\r\n    public Color trajectoryColor = Color.blue;\r\n    public float trajectoryWidth = 0.1f;\r\n\r\n    private LineRenderer trajectoryLine;\r\n    private float lastUpdateTime;\r\n\r\n    void Start()\r\n    {\r\n        positionBuffer = new Queue<Vector3>();\r\n        rotationBuffer = new Queue<Quaternion>();\r\n        jointStateBuffer = new Queue<float[]>();\r\n\r\n        SetupTrajectoryVisualization();\r\n        lastUpdateTime = Time.time;\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        if (isLiveConnection && Time.time - lastUpdateTime > 1.0f / updateRate)\r\n        {\r\n            UpdateFromLiveData();\r\n            lastUpdateTime = Time.time;\r\n        }\r\n\r\n        UpdateVisualization();\r\n    }\r\n\r\n    void SetupTrajectoryVisualization()\r\n    {\r\n        if (showTrajectory)\r\n        {\r\n            trajectoryLine = gameObject.AddComponent<LineRenderer>();\r\n            trajectoryLine.material = new Material(Shader.Find("Sprites/Default"));\r\n            trajectoryLine.color = trajectoryColor;\r\n            trajectoryLine.startWidth = trajectoryWidth;\r\n            trajectoryLine.endWidth = trajectoryWidth;\r\n        }\r\n    }\r\n\r\n    void UpdateFromLiveData()\r\n    {\r\n        // In a real implementation, this would receive data from ROS\r\n        // For demonstration, we\'ll simulate data updates\r\n        SimulateLiveDataUpdate();\r\n    }\r\n\r\n    void SimulateLiveDataUpdate()\r\n    {\r\n        // Simulate receiving position, rotation, and joint state data\r\n        Vector3 newPosition = robotAvatar.transform.position +\r\n                             new Vector3(Random.Range(-0.1f, 0.1f),\r\n                                        0,\r\n                                        Random.Range(-0.1f, 0.1f));\r\n\r\n        Quaternion newRotation = robotAvatar.transform.rotation *\r\n                                Quaternion.Euler(0, Random.Range(-5f, 5f), 0);\r\n\r\n        float[] jointStates = new float[6];\r\n        for (int i = 0; i < jointStates.Length; i++)\r\n        {\r\n            jointStates[i] = Random.Range(-1.5f, 1.5f); // Simulated joint angles\r\n        }\r\n\r\n        // Add to buffers\r\n        AddToBuffer(positionBuffer, newPosition);\r\n        AddToBuffer(rotationBuffer, newRotation);\r\n        AddToBuffer(jointStateBuffer, jointStates);\r\n\r\n        // Update robot avatar with new data\r\n        robotAvatar.targetPosition = newPosition;\r\n        robotAvatar.targetRotation = newRotation;\r\n        robotAvatar.UpdateJointPositions(jointStates);\r\n\r\n        // Update trajectory visualization\r\n        if (showTrajectory && trajectoryLine != null)\r\n        {\r\n            trajectoryLine.positionCount = positionBuffer.Count;\r\n            trajectoryLine.SetPositions(positionBuffer.ToArray());\r\n        }\r\n    }\r\n\r\n    void AddToBuffer<T>(Queue<T> buffer, T item)\r\n    {\r\n        buffer.Enqueue(item);\r\n        if (buffer.Count > maxBufferSize)\r\n            buffer.Dequeue();\r\n    }\r\n\r\n    void UpdateVisualization()\r\n    {\r\n        // Update any visualization elements based on current data\r\n        // This could include updating graphs, status displays, etc.\r\n    }\r\n\r\n    // Method to manually update with live data from ROS\r\n    public void UpdateWithRealData(Vector3 position, Quaternion rotation, float[] jointStates)\r\n    {\r\n        AddToBuffer(positionBuffer, position);\r\n        AddToBuffer(rotationBuffer, rotation);\r\n        AddToBuffer(jointStateBuffer, jointStates);\r\n\r\n        robotAvatar.targetPosition = position;\r\n        robotAvatar.targetRotation = rotation;\r\n        robotAvatar.UpdateJointPositions(jointStates);\r\n\r\n        if (showTrajectory && trajectoryLine != null)\r\n        {\r\n            trajectoryLine.positionCount = positionBuffer.Count;\r\n            trajectoryLine.SetPositions(positionBuffer.ToArray());\r\n        }\r\n    }\r\n}\n'})}),"\n",(0,i.jsx)(e.h2,{id:"summary--key-takeaways",children:"Summary & Key Takeaways"}),"\n",(0,i.jsx)(e.p,{children:"In this chapter, you've learned about Unity's role in robotics and human-robot interaction:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Unity's value in robotics"})," lies in its superior graphics, visualization capabilities, and user interface design tools"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Graphics vs physics trade-offs"})," require choosing the right tool for specific application needs"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"ROS-Unity communication"})," involves specialized middleware and message translation systems"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human-robot interaction design"})," benefits from Unity's intuitive interface creation tools"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"You've seen practical examples of creating Unity scenes with robot avatars, implementing keyboard and gesture-based controls, and building visualization-only digital twins. Unity complements physics-focused simulators like Gazebo by providing advanced visualization and interaction capabilities that are essential for testing human-robot interaction scenarios and creating compelling demonstrations."}),"\n",(0,i.jsx)(e.p,{children:"In the next chapter, we'll explore sim-to-real validation strategies that help ensure your simulation results translate effectively to real-world robot deployment."})]})}function u(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>a,x:()=>s});var r=t(6540);const i={},o=r.createContext(i);function a(n){const e=r.useContext(o);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),r.createElement(o.Provider,{value:e},n.children)}}}]);