"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[548],{3428(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-08-capstone-autonomous-humanoid/voice-to-plan","title":"Chapter 2: Voice-to-Plan Pipeline","description":"Introduction","source":"@site/docs/module-08-capstone-autonomous-humanoid/voice-to-plan.md","sourceDirName":"module-08-capstone-autonomous-humanoid","slug":"/module-08-capstone-autonomous-humanoid/voice-to-plan","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-08-capstone-autonomous-humanoid/voice-to-plan","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadNadeem92/Physical-AI-Humanoid-Robotics-TextBook/tree/main/docs/module-08-capstone-autonomous-humanoid/voice-to-plan.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: System Architecture & Data Flow","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-08-capstone-autonomous-humanoid/system-architecture"},"next":{"title":"Chapter 3: Perception & Grounding","permalink":"/Physical-AI-Humanoid-Robotics-TextBook/docs/module-08-capstone-autonomous-humanoid/perception-grounding"}}');var s=i(4848),o=i(8453);const r={},a="Chapter 2: Voice-to-Plan Pipeline",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Speech-to-Text Processing",id:"speech-to-text-processing",level:3},{value:"Intent Parsing",id:"intent-parsing",level:3},{value:"Task Schemas",id:"task-schemas",level:3},{value:"Ambiguity Resolution",id:"ambiguity-resolution",level:3},{value:"Examples",id:"examples",level:2},{value:"Example: Voice Command \u2192 JSON Task Plan",id:"example-voice-command--json-task-plan",level:3},{value:"Example: Validation Rules",id:"example-validation-rules",level:3},{value:"Example: Voice Processing Pipeline",id:"example-voice-processing-pipeline",level:3},{value:"Summary &amp; Key Takeaways",id:"summary--key-takeaways",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-2-voice-to-plan-pipeline",children:"Chapter 2: Voice-to-Plan Pipeline"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"This chapter focuses on transforming natural language voice commands into structured task plans that can be executed by the autonomous humanoid system. You'll learn how to connect voice processing with LLM reasoning to create robust pipelines that convert human intent into executable actions."}),"\n",(0,s.jsxs)(n.p,{children:["The voice-to-plan pipeline represents a critical integration point between human communication and robotic action. This chapter builds on the voice processing concepts from ",(0,s.jsx)(n.a,{href:"../module-06-vla-systems/voice-language",children:"Module 6"})," and connects them with the LLM reasoning concepts from ",(0,s.jsx)(n.a,{href:"../module-05-isaac-ai-brain/learning-sim2real",children:"Module 5"}),". This chapter emphasizes the connection between language understanding and physical execution, ensuring that abstract commands are translated into concrete, safe actions. The pipeline must handle ambiguity, validate intentions, and create structured plans that the rest of the system can execute reliably."]}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you'll understand how to create voice-to-plan pipelines that connect human intent to robotic action while maintaining safety and reliability throughout the translation process."}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"speech-to-text-processing",children:"Speech-to-Text Processing"}),"\n",(0,s.jsx)(n.p,{children:"The initial conversion of voice commands into textual form that can be processed by LLMs and other components."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Processing Steps:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Capture"}),": Collect voice input with appropriate noise filtering"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transcription"}),": Convert speech to text using models like Whisper"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quality Assessment"}),": Evaluate transcription confidence and clarity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Preprocessing"}),": Clean and format text for LLM processing"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Quality Considerations:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise Filtering"}),": Remove background noise and interference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speaker Isolation"}),": Focus on primary speaker in multi-person environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Confidence Scoring"}),": Assess reliability of transcription results"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Preservation"}),": Maintain temporal and semantic context"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"intent-parsing",children:"Intent Parsing"}),"\n",(0,s.jsx)(n.p,{children:"The process of extracting actionable intent from transcribed text using LLMs and structured analysis."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parsing Components:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Entity Recognition"}),": Identify objects, locations, and actions in the command"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relationship Mapping"}),": Understand spatial and temporal relationships"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Decomposition"}),": Break complex commands into executable steps"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Constraint Extraction"}),": Identify safety and execution constraints"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parsing Strategies:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Schema-Based"}),": Use predefined templates for common command types"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM-Based"}),": Leverage large language models for flexible understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validation Layer"}),": Cross-check parsed intent with available capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ambiguity Resolution"}),": Handle unclear or conflicting information"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"task-schemas",children:"Task Schemas"}),"\n",(0,s.jsx)(n.p,{children:"Structured representations of tasks that can be validated and executed by the system."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Schema Components:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Sequence"}),": Ordered list of actions to execute"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object References"}),": Specific objects to interact with"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Location Constraints"}),": Where actions should occur"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Boundaries"}),": Constraints to ensure safe execution"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Schema Validation:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feasibility Check"}),": Verify tasks are physically possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Validation"}),": Ensure no safety boundaries are violated"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Availability"}),": Confirm required resources are available"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dependency Resolution"}),": Order actions appropriately"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ambiguity-resolution",children:"Ambiguity Resolution"}),"\n",(0,s.jsx)(n.p,{children:"Strategies for handling unclear or ambiguous voice commands that require clarification."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Resolution Approaches:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context-Based"}),": Use environmental context to resolve ambiguity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception-Based"}),": Use vision systems to identify ambiguous objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Query-Based"}),": Request clarification from the human user"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Default-Based"}),": Apply safe defaults when ambiguity cannot be resolved"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Safety Considerations:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Conservative Defaults"}),": Choose safe options when uncertain"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"User Confirmation"}),": Request approval for potentially risky interpretations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback Strategies"}),": Provide alternative execution paths when ambiguous"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.h3,{id:"example-voice-command--json-task-plan",children:"Example: Voice Command \u2192 JSON Task Plan"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Voice Command:"}),' "Please go to the kitchen table and bring me the red mug from there to the living room couch."']}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Processing Pipeline:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'1. Speech-to-Text:\r\n   Input: Audio "Please go to the kitchen table and bring me the red mug from there to the living room couch."\r\n   Output: "Please go to the kitchen table and bring me the red mug from there to the living room couch."\r\n\r\n2. Intent Parsing:\r\n   - Action: Navigate \u2192 Grasp \u2192 Navigate \u2192 Place\r\n   - Source: kitchen table, red mug\r\n   - Destination: living room couch\r\n   - Object: red mug\r\n\r\n3. Task Schema Generation:\r\n   {\r\n     "id": "task_001",\r\n     "actions": [\r\n       {\r\n         "type": "navigate",\r\n         "target": "kitchen table",\r\n         "constraints": {"max_distance": 5.0, "safety_radius": 1.0}\r\n       },\r\n       {\r\n         "type": "locate",\r\n         "object": "red mug",\r\n         "reference": "kitchen table",\r\n         "constraints": {"search_timeout": 30, "confidence_threshold": 0.8}\r\n       },\r\n       {\r\n         "type": "grasp",\r\n         "object": "red mug",\r\n         "constraints": {"grasp_type": "precision", "force_limit": 10.0}\r\n       },\r\n       {\r\n         "type": "navigate",\r\n         "target": "living room couch",\r\n         "constraints": {"max_distance": 10.0, "safety_radius": 1.0}\r\n       },\r\n       {\r\n         "type": "place",\r\n         "object": "red mug",\r\n         "target": "living room couch",\r\n         "constraints": {"placement_type": "safe", "height": 0.5}\r\n       }\r\n     ],\r\n     "safety_boundaries": ["human_safety_zone", "navigation_obstacles"],\r\n     "validation_requirements": ["object_existence", "path_clearance"]\r\n   }\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-validation-rules",children:"Example: Validation Rules"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Validation Rule Set for Voice-to-Plan Pipeline:\r\n\r\n1. Object Existence Validation:\r\n   - Before: "red mug" must exist in environment\r\n   - How: Use perception system to locate object\r\n   - If Missing: Report error, suggest alternatives\r\n\r\n2. Path Feasibility Validation:\r\n   - Before: Navigation paths must be clear\r\n   - How: Use navigation system to plan path\r\n   - If Blocked: Find alternative route or abort\r\n\r\n3. Safety Boundary Validation:\r\n   - Before: No safety boundaries violated\r\n   - How: Check human safety zones, restricted areas\r\n   - If Violated: Abort and report safety concern\r\n\r\n4. Capability Validation:\r\n   - Before: Robot can perform requested actions\r\n   - How: Check manipulation and navigation capabilities\r\n   - If Unsupported: Report capability limitation\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-voice-processing-pipeline",children:"Example: Voice Processing Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[Voice Input] \u2192 [Audio Preprocessing] \u2192 [Speech-to-Text] \u2192 [Text Validation]\r\n      \u2193               \u2193                      \u2193                \u2193\r\n[Noise Filter]  [Volume Normalization]  [Confidence Score] [Format Cleaning]\r\n\r\n[Intent Parsing] \u2192 [Entity Recognition] \u2192 [Task Decomposition] \u2192 [Schema Validation]\r\n      \u2193                 \u2193                      \u2193                    \u2193\r\n[LLM Processing]  [Object/Location Tags]  [Action Sequence]   [Safety Check]\r\n\r\n[Task Plan Output] \u2192 [Execution Readiness]\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary--key-takeaways",children:"Summary & Key Takeaways"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, you learned about the voice-to-plan pipeline:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech-to-text processing"})," converts voice commands to textual form for further processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intent parsing"})," extracts actionable meaning from natural language using LLMs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task schemas"})," provide structured representations that can be validated and executed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ambiguity resolution"})," handles unclear commands with safety-first approaches"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The voice-to-plan pipeline serves as the bridge between human communication and robotic action, requiring careful attention to safety, validation, and reliability. This chapter connects the voice processing concepts from earlier modules with the action execution capabilities that will be covered in later chapters."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);